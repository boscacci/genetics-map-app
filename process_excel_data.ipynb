{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Excel Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# Set display options for better viewing\n",
    "pd.set_option('display.max_columns', 17)\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "# pd.set_option('display.width', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "excel_file = 'moni_data_2025-09-28.xlsx'\n",
    "\n",
    "# Read the Excel file - you can specify sheet name if needed\n",
    "# df = pd.read_excel(excel_file, sheet_name='Sheet1')  # or specify the exact sheet name\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "print(f\"Loaded Excel file: {excel_file}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually map verbose column names to short, intuitive snake_case names\n",
    "\n",
    "column_map = {\n",
    "    \"First Name\": \"first_name\",\n",
    "    \"Last Name\": \"last_name\",\n",
    "    \"Email Address\": \"email\",\n",
    "    \"Office/Work Phone number (please provide with area code) Eg: +1 for USA, +91 for India\": \"phone\",\n",
    "    \"Link to your Work Website\": \"website\",\n",
    "    \"Name of your institution(s)/work place(s)\": \"institution\",\n",
    "    \"Address of your institution(s)/work place(s)\": \"address\",\n",
    "    \" Language(s) are able to provide genetics/genetic counsel(l)ing services in\": \"languages\",\n",
    "    \"Specialties\": \"specialties\"\n",
    "}\n",
    "\n",
    "# Only keep columns that are in the mapping\n",
    "df = df[[col for col in column_map.keys() if col in df.columns]].copy()\n",
    "\n",
    "# Rename columns in the dataframe\n",
    "df = df.rename(columns=column_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import validators\n",
    "\n",
    "# Clean up emails: strip, lowercase, remove commas and spaces\n",
    "emails_cleaned = (\n",
    "    df['email']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .str.replace(' ', '', regex=False)\n",
    ")\n",
    "\n",
    "# If multiple emails in a cell, split and take the first valid one\n",
    "def first_valid_email(cell):\n",
    "    for e in str(cell).split(';'):\n",
    "        e = e.strip()\n",
    "        try:\n",
    "            if validators.email(e):\n",
    "                return e\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "df['email'] = emails_cleaned.apply(first_valid_email)\n",
    "\n",
    "# Show any still-invalid emails\n",
    "def is_invalid_email(x):\n",
    "    try:\n",
    "        return not bool(validators.email(x))\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "invalid_emails = df['email'][df['email'].fillna('').apply(is_invalid_email)]\n",
    "print(\"Invalid email addresses after cleaning:\")\n",
    "invalid_emails.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.email.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show bar chart of non-null counts per column (info equivalent)\n",
    "non_null_counts = df.notnull().sum()\n",
    "plt.figure(figsize=(10, 5))\n",
    "non_null_counts.plot(kind='bar')\n",
    "plt.title('Non-null Value Count per Column')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Column')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show bar charts of basic statistics (describe equivalent) for numeric columns\n",
    "desc = df.describe().T\n",
    "if not desc.empty:\n",
    "    stats_to_plot = ['mean', 'std', 'min', 'max']\n",
    "    available_stats = [stat for stat in stats_to_plot if stat in desc.columns]\n",
    "    if available_stats:\n",
    "        desc[available_stats].plot(kind='bar', subplots=True, layout=(2,2), figsize=(12,8), legend=False)\n",
    "        plt.suptitle('Basic Statistics for Numeric Columns')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No numeric statistics ('mean', 'std', 'min', 'max') available to plot.\")\n",
    "else:\n",
    "    print(\"No numeric columns to describe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up selected fields: strip whitespace, collapse spaces, and handle newlines in address\n",
    "fields_to_clean = ['first_name', 'last_name', 'institution', 'address']\n",
    "for col in fields_to_clean:\n",
    "    if col in df.columns:\n",
    "        s = df[col].astype(str).str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "        if col == 'address':\n",
    "            s = s.str.replace(r'\\n+', ', ', regex=True)\n",
    "        df[col] = s\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import validators\n",
    "\n",
    "non_url_responses = {\n",
    "    'prefer not to say', 'not available', 'i do not have a work website',\n",
    "    'none', 'na', 'n/a', '', 'being updated', 'retired', 'n.a.', 'n.a', 'n.a'\n",
    "}\n",
    "\n",
    "# Stats counters\n",
    "website_stats = {\n",
    "    'set_to_nan': 0,\n",
    "    'truncated': 0,\n",
    "    'fixed_space_split': 0,\n",
    "    'normalized_www': 0,\n",
    "    'extracted_url': 0,\n",
    "    'removed_trailing_slash': 0,\n",
    "    'prepended_https': 0,\n",
    "    'could_not_parse': 0,\n",
    "}\n",
    "\n",
    "def clean_website_stats(val):\n",
    "    \"\"\"\n",
    "    Clean website field, keep stats of changes, lowercase before processing.\n",
    "    Only return the first valid URL or domain. Fixes edge case where a space splits a domain.\n",
    "    \"\"\"\n",
    "    if pd.isnull(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if s in non_url_responses:\n",
    "        website_stats['set_to_nan'] += 1\n",
    "        return np.nan\n",
    "\n",
    "    # Remove trailing punctuation and spaces\n",
    "    s = s.rstrip(' .,/;:\\n\\t')\n",
    "\n",
    "    # If there are multiple entries separated by ; or newline, only keep the first\n",
    "    if re.search(r'[;\\n]', s):\n",
    "        first = re.split(r'[;\\n]', s)[0].strip()\n",
    "        website_stats['truncated'] += 1\n",
    "    else:\n",
    "        first = s\n",
    "\n",
    "    first = first.strip().lower()\n",
    "    if first in non_url_responses:\n",
    "        website_stats['set_to_nan'] += 1\n",
    "        return np.nan\n",
    "\n",
    "    # Remove trailing punctuation and spaces again after truncation\n",
    "    first = first.rstrip(' .,/;:\\n\\t')\n",
    "\n",
    "    # Fix edge case: if the string starts with http(s):// or www. but contains a space, try to join the next word if it looks like a domain\n",
    "    if re.match(r'^(https?://|www\\.)', first) and ' ' in first:\n",
    "        words = first.split()\n",
    "        prefix_match = re.match(r'^(https?://|www\\.)', words[0])\n",
    "        prefix = prefix_match.group(1) if prefix_match else ''\n",
    "        rest = ''.join(words)\n",
    "        if prefix and rest.startswith(prefix):\n",
    "            rest = rest[len(prefix):]\n",
    "        candidate = prefix + rest\n",
    "        domain_pattern = r'^[A-Za-z0-9\\-\\.]+\\.[A-Za-z]{2,}([/\\w\\-\\.\\?\\=\\&\\%]*)?$'\n",
    "        if re.search(domain_pattern, candidate, re.IGNORECASE):\n",
    "            website_stats['fixed_space_split'] += 1\n",
    "            first = candidate\n",
    "\n",
    "    # Try to extract a URL or www. domain (allow for spaces in URLs, but only take up to first space)\n",
    "    url_match = re.findall(r'((?:https?://|www\\.)[^\\s,;]+)', first)\n",
    "    if url_match:\n",
    "        url = url_match[0]\n",
    "        url = url.rstrip(' .,/;:\\n\\t')\n",
    "        if url.startswith('www.'):\n",
    "            website_stats['normalized_www'] += 1\n",
    "            url = 'https://' + url\n",
    "        elif url != first:\n",
    "            website_stats['extracted_url'] += 1\n",
    "        return url.rstrip('/')\n",
    "\n",
    "    # If it looks like a domain (contains a dot, no spaces), prepend https://\n",
    "    domain_pattern = r'^[A-Za-z0-9\\-\\.]+\\.[A-Za-z]{2,}([/\\w\\-\\.\\?\\=\\&\\%]*)?$'\n",
    "    if '.' in first and ' ' not in first:\n",
    "        if first.startswith(('http://', 'https://')):\n",
    "            if first.endswith('/'):\n",
    "                website_stats['removed_trailing_slash'] += 1\n",
    "            return first.rstrip('/')\n",
    "        if re.match(domain_pattern, first, re.IGNORECASE):\n",
    "            website_stats['prepended_https'] += 1\n",
    "            return 'https://' + first.rstrip('/')\n",
    "    # If it looks like a domain with spaces (e.g. 'ahs.ca/genetics'), try to extract the domain part\n",
    "    if '.' in first and '/' in first and ' ' in first:\n",
    "        domain_candidate = first.split(' ')[0]\n",
    "        if re.match(domain_pattern, domain_candidate, re.IGNORECASE):\n",
    "            website_stats['prepended_https'] += 1\n",
    "            return 'https://' + domain_candidate.rstrip('/')\n",
    "\n",
    "    website_stats['could_not_parse'] += 1\n",
    "    return first\n",
    "\n",
    "if 'website' in df.columns:\n",
    "    orig = df['website'].copy()\n",
    "    df['website'] = df['website'].apply(clean_website_stats)\n",
    "\n",
    "    # Show value counts for values that became null\n",
    "    turned_null = orig[~orig.isnull() & df['website'].isnull()]\n",
    "    if not turned_null.empty:\n",
    "        print(\"Values turned into nulls in 'website':\")\n",
    "        print(turned_null.value_counts())\n",
    "\n",
    "    # Now check for validity using validators\n",
    "    cleaned = df['website'].dropna().unique()\n",
    "    invalid = []\n",
    "    for val in cleaned:\n",
    "        # Accept LinkedIn and Google as special cases\n",
    "        if isinstance(val, str) and (\n",
    "            'linkedin.com' in val or\n",
    "            'google.com' in val or\n",
    "            val == 'linkedin' or\n",
    "            val == 'google'\n",
    "        ):\n",
    "            continue\n",
    "        try:\n",
    "            if validators.url(val) or validators.domain(val):\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            invalid.append(val)\n",
    "            continue\n",
    "        invalid.append(val)\n",
    "    if invalid:\n",
    "        print(\"Website values thrown out by validators (these are kept in the data, but flagged as invalid):\")\n",
    "        for v in invalid:\n",
    "            print(\"  \", v)\n",
    "    else:\n",
    "        print(\"All cleaned website values are valid.\")\n",
    "\n",
    "    # Print stats summary\n",
    "    print(\"\\nWebsite cleaning stats:\")\n",
    "    for k, v in website_stats.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.website.sample(15).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_phone(val):\n",
    "    if pd.isnull(val):\n",
    "        return None\n",
    "    val_str = str(val).strip()\n",
    "    if val_str.lower() == 'prefer not to say' or val_str == '':\n",
    "        return None\n",
    "\n",
    "    # If value contains a slash, try to extract the phone number part\n",
    "    if '/' in val_str:\n",
    "        # Split on slash and look for a phone-like pattern in each part\n",
    "        parts = [p.strip() for p in val_str.split('/')]\n",
    "        phone_candidate = None\n",
    "        for part in parts:\n",
    "            # Look for a sequence of digits, possibly with +, spaces, dashes, or parentheses\n",
    "            if re.search(r'(\\+?\\d[\\d\\s\\-\\(\\)]{5,})', part):\n",
    "                phone_candidate = part\n",
    "                break\n",
    "        if phone_candidate:\n",
    "            val_str = phone_candidate\n",
    "\n",
    "    # Remove any leading/trailing whitespace again\n",
    "    val_str = val_str.strip()\n",
    "    # Remove double spaces\n",
    "    val_str = re.sub(r' {2,}', ' ', val_str)\n",
    "    if val_str == '':\n",
    "        return None\n",
    "    return val_str\n",
    "\n",
    "df['phone'] = df['phone'].apply(clean_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.phone.sample(20).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def uses_interpreters_func(val):\n",
    "    if pd.isnull(val):\n",
    "        return False\n",
    "\n",
    "    val_str = str(val).lower()\n",
    "\n",
    "    # Phrases that indicate interpreter/translation services\n",
    "    interpreter_patterns = [\n",
    "        r'interpret(er|ation)( services| present| available)?',\n",
    "        r'with use of',\n",
    "        r'with provided interpreter',\n",
    "        r'others? with (provided )?interpreter( services)?',\n",
    "        r'all other languages with interpretation services',\n",
    "        r'we also use interpreters?',\n",
    "        r'globo interpreter',\n",
    "        r'translation',\n",
    "        r'interpreter present',\n",
    "        r'other languages with an interpreter present',\n",
    "    ]\n",
    "    for pat in interpreter_patterns:\n",
    "        if re.search(pat, val_str):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_languages(val):\n",
    "    if pd.isnull(val):\n",
    "        return None\n",
    "\n",
    "    # Convert to string and strip whitespace\n",
    "    val = str(val).strip()\n",
    "\n",
    "    # Remove parentheses and their contents\n",
    "    val = re.sub(r'\\([^)]*\\)', '', val)\n",
    "\n",
    "    # Remove \"prefer not to say\" and similar\n",
    "    if re.search(r'prefer not to say', val, re.IGNORECASE):\n",
    "        return None\n",
    "\n",
    "    # Remove phrases indicating interpreter/translation services\n",
    "    val = re.sub(\n",
    "        r'(all other languages with interpretation services|others? with (provided )?interpreter( services)?|interpretation services available|we also use interpreters?|limited [a-z]+|with use of [A-Za-z ]+|other languages with an interpreter present|interpreter present|translation|globo interpreter)',\n",
    "        '', val, flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Remove trailing/leading whitespace again after phrase removal\n",
    "    val = val.strip()\n",
    "\n",
    "    # Normalize separators: replace ; | / and newlines with commas\n",
    "    val = re.sub(r'[;|/]', ',', val)\n",
    "    val = val.replace('\\n', ',')\n",
    "\n",
    "    # Replace \" and \", \" & \", \" and\", \"&\", \"and \" (with or without spaces) with comma\n",
    "    val = re.sub(r'(\\s+and\\s+|\\s+&\\s+|^and\\s+|\\s+and$|^&\\s+|\\s+&$)', ',', val, flags=re.IGNORECASE)\n",
    "    val = re.sub(r'^(and|&)\\s+', ',', val, flags=re.IGNORECASE)\n",
    "    val = re.sub(r'\\s+(and|&)$', ',', val, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove periods (special characters) except those in abbreviations (e.g., \"U.S.A.\" becomes \"USA\")\n",
    "    val = re.sub(r'\\.', '', val)\n",
    "\n",
    "    # Remove any other special characters except commas, letters, and spaces\n",
    "    val = re.sub(r'[^a-zA-Z,\\s]', '', val)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    val = re.sub(r'\\s+', ' ', val)\n",
    "\n",
    "    # Remove empty commas and leading/trailing commas\n",
    "    val = re.sub(r',+', ',', val)\n",
    "    val = val.strip(',')\n",
    "    val = val.strip()\n",
    "\n",
    "    # Sometimes people write languages without commas, e.g. \"Kannada English Hindi\"\n",
    "    # If there are 2+ words with no commas, split them\n",
    "    # First, split by comma, then further split by space if the entry contains multiple words and no comma\n",
    "    langs = []\n",
    "    for chunk in val.split(','):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        # If chunk contains multiple words, split them\n",
    "        words = chunk.split()\n",
    "        if len(words) > 1:\n",
    "            langs.extend(words)\n",
    "        else:\n",
    "            langs.append(chunk)\n",
    "\n",
    "    # Remove any that are just \"other\", \"others\", \"interpreter\", \"interpreter present\", \"interpretation\", \"translation\", \"services\", \"provided\", \"available\", \"languages\", \"with\", \"an\", \"present\", \"some\"\n",
    "    ignore = {\n",
    "        'other', 'others', 'interpreter', 'interpreter present', 'interpretation', 'translation',\n",
    "        'services', 'provided', 'available', 'languages', 'with', 'an', 'present', 'some'\n",
    "    }\n",
    "    langs = [l for l in langs if l.lower() not in ignore and l.strip()]\n",
    "\n",
    "    # Do NOT try to interpret shortcodes for languages or filter out single-character codes\n",
    "\n",
    "    # Capitalize each language (special handling for multi-word languages is not needed now)\n",
    "    langs = [l.strip().capitalize() for l in langs]\n",
    "\n",
    "    # Remove duplicates while preserving order (case-insensitive)\n",
    "    seen = set()\n",
    "    langs_clean = []\n",
    "    for l in langs:\n",
    "        l_key = l.lower()\n",
    "        if l_key not in seen:\n",
    "            langs_clean.append(l)\n",
    "            seen.add(l_key)\n",
    "\n",
    "    # Always sort the languages alphabetically (case-insensitive)\n",
    "    langs_clean_sorted = sorted(langs_clean, key=lambda x: x.lower())\n",
    "\n",
    "    return ', '.join(langs_clean_sorted) if langs_clean_sorted else None\n",
    "\n",
    "# Keep the original languages column for comparison\n",
    "df['languages_original'] = df['languages']\n",
    "\n",
    "df['uses_interpreters'] = df['languages'].apply(uses_interpreters_func)\n",
    "df['languages'] = df['languages'].apply(clean_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email languages_original languages uses_interpreters'.split()].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['languages_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.languages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move \"specialties\" to the last column\n",
    "cols = [col for col in df.columns if col != 'specialties'] + ['specialties']\n",
    "df = df[cols].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns before export\n",
    "df_export = df.rename(columns={\n",
    "    'first_name': 'name_first',\n",
    "    'last_name': 'name_last',\n",
    "    'email': 'email',\n",
    "    'phone': 'phone_work',\n",
    "    'website': 'work_website',\n",
    "    'institution': 'work_institution',\n",
    "    'address': 'work_address',\n",
    "    'languages': 'language_spoken',\n",
    "    'uses_interpreters': 'uses_interpreters',\n",
    "    'specialties': 'specialties'\n",
    "})\n",
    "\n",
    "# Export CSV with quoting to ensure Excel/Google Sheets parse fields correctly\n",
    "import csv\n",
    "df_export.to_csv(\n",
    "    \"data_for_geocoding.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,  # Quote all fields to prevent misparsing\n",
    "    encoding=\"utf-8-sig\"    # Add BOM for Excel compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.query(\"uses_interpreters == True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
